\documentclass{beamer}

\mode<presentation>
{
  \usetheme{default}
  \usecolortheme{default}
  \usefonttheme{default}
  \setbeamertemplate{navigation symbols}{}
  \setbeamertemplate{caption}[numbered]
  \setbeamertemplate{footline}[page number]
  \setbeamercolor{frametitle}{fg=white}
  \setbeamercolor{footline}{fg=black}
} 

\usepackage[english]{babel}
\usepackage[utf8x]{inputenc}
\usepackage{tikz}
\usepackage{listings}
\usepackage{courier}
\usepackage{array}
\usepackage{bold-extra}
\usepackage{minted}
\usepackage{ulem}
\usepackage{xpatch}

\xdefinecolor{darkblue}{rgb}{0.1,0.1,0.7}
\xdefinecolor{darkgreen}{rgb}{0,0.5,0}
\xdefinecolor{darkgrey}{rgb}{0.35,0.35,0.35}
\xdefinecolor{darkorange}{rgb}{0.8,0.5,0}
\xdefinecolor{darkred}{rgb}{0.7,0,0}
\xdefinecolor{dianablue}{rgb}{0.18,0.24,0.31}
\definecolor{commentgreen}{rgb}{0,0.6,0}
\definecolor{stringmauve}{rgb}{0.58,0,0.82}

\lstset{ %
  backgroundcolor=\color{white},      % choose the background color
  basicstyle=\ttfamily\small,         % size of fonts used for the code
  breaklines=true,                    % automatic line breaking only at whitespace
  captionpos=b,                       % sets the caption-position to bottom
  commentstyle=\color{commentgreen},  % comment style
  escapeinside={\%*}{*)},             % if you want to add LaTeX within your code
  keywordstyle=\color{blue},          % keyword style
  stringstyle=\color{stringmauve},    % string literal style
  showstringspaces=false,
  showlines=true
}

\lstdefinelanguage{scala}{
  morekeywords={abstract,case,catch,class,def,%
    do,else,extends,false,final,finally,%
    for,if,implicit,import,match,mixin,%
    new,null,object,override,package,%
    private,protected,requires,return,sealed,%
    super,this,throw,trait,true,try,%
    type,val,var,while,with,yield},
  otherkeywords={=>,<-,<\%,<:,>:,\#,@},
  sensitive=true,
  morecomment=[l]{//},
  morecomment=[n]{/*}{*/},
  morestring=[b]",
  morestring=[b]',
  morestring=[b]"""
}

\title[2017-08-22-acat-hepquery]{Toward real-time data query systems in HEP}
\author{Jim Pivarski}
\institute{Princeton University -- DIANA}
\date{August 22, 2017}

\xpatchcmd{\sout}
  {\bgroup}
  {\bgroup\def\ULthickness{1.5 pt}}
  {}{}

\begin{document}

\logo{\pgfputat{\pgfxy(0.11, 8)}{\pgfbox[right,base]{\tikz{\filldraw[fill=dianablue, draw=none] (0 cm, 0 cm) rectangle (50 cm, 1 cm);}}}\pgfputat{\pgfxy(0.11, -0.6)}{\pgfbox[right,base]{\tikz{\filldraw[fill=dianablue, draw=none] (0 cm, 0 cm) rectangle (50 cm, 1 cm);}\includegraphics[height=0.99 cm]{diana-hep-logo.png}\tikz{\filldraw[fill=dianablue, draw=none] (0 cm, 0 cm) rectangle (4.9 cm, 1 cm);}}}}

\begin{frame}
  \titlepage
\end{frame}

\logo{\pgfputat{\pgfxy(0.11, 8)}{\pgfbox[right,base]{\tikz{\filldraw[fill=dianablue, draw=none] (0 cm, 0 cm) rectangle (50 cm, 1 cm);}\includegraphics[height=1 cm]{diana-hep-logo.png}}}}

% Uncomment these lines for an automatically generated outline.
%\begin{frame}{Outline}
%  \tableofcontents
%\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{The dream}
\vspace{0.5 cm}
LHC experiments are pretty efficient at data handling up to Analysis Object Datasets (AODs), but AODs to final plots is a wild west of hacky scripts, unnecessary copying, and wasted resources.

\vspace{0.2 cm}
\only<1>{\includegraphics[width=\linewidth]{stack-of-files.png}}\only<2>{\includegraphics[width=\linewidth]{stack-of-files-2.png}}

\vspace{0.5 cm}
\begin{uncoverenv}<2>
Ideally, we'd want to turn AOD directly into plots, quickly enough for interactive analysis (seconds per scan at most).
\end{uncoverenv}
\end{frame}

\begin{frame}{Motivation}
\large
\vspace{0.5 cm}
\begin{itemize}\setlength{\itemsep}{0.5 cm}
\item Users would plot the data before doing anything else.

\vspace{0.2 cm}
\textcolor{gray}{\normalsize (Skims for maximum likelihood fits, etc.\ are still possible, but would be more streamlined {\it after} exploratory plotting.)}

\vspace{-0.2 cm}
\item Focus on physics and statistical issues, not data handling.
\item Facilitates provenance and analysis reproducibility.
\item CPU, disk, and memory would be more efficiently used.
\item Small institutions would not be ``priced out'' of analysis for lack of resources to copy and locally process the data.
\end{itemize}
\end{frame}

\begin{frame}{Existence proof}

In some industries, it is possible to ``process petabyes of data and trillions of records in seconds\footnotemark[1].''

\vspace{0.2 cm}
In fact, there are many low-latency query server engines available, mostly open source.

\vspace{0.5 cm}
\mbox{\hspace{-1 cm}
\includegraphics[height=1.23 cm]{drill-logo.png}
\includegraphics[height=1.23 cm]{impala-logo.png}
\includegraphics[height=1.23 cm]{kudu-logo.png}
\includegraphics[height=1.23 cm]{ibis-logo.png}
\includegraphics[height=1.23 cm]{hawk-logo.png}
\includegraphics[height=1.23 cm]{dremel-logo.png}
\includegraphics[height=1.23 cm]{bigquery-logo.png}
\includegraphics[height=1.23 cm]{sparksql-logo.png}}

\vspace{0.5 cm}
\begin{uncoverenv}<2>
Apache Drill comes closest to fitting our needs, but it's
\begin{itemize}
\item SQL (not expressive enough for HEP)
\item Java (hard to link to HEP software)
\item more suited to ``flat ntuple'' analysis.
\end{itemize}
\end{uncoverenv}

\footnotetext[1]{\textcolor{blue}{\url{https://wiki.apache.org/incubator/DrillProposal}}}
\end{frame}

%% \begin{frame}{Existence proof}
%% \vspace{0.5 cm}
%% \mbox{ } \hfill \includegraphics[height=2 cm]{drill-logo.png} \hfill \mbox{ }

%% \begin{columns}
%% \column{0.5\linewidth}
%% \textcolor{darkblue}{\large \underline{What's right}}

%% \begin{itemize}
%% \item lightweight parallelization
%% \item columnar data
%% \item just-in-time (JIT) compilation, vectorization
%% \item data stay on the server, only summaries come down
%% \item extensible: plug-in architecture
%% \end{itemize}

%% \column{0.5\linewidth}
%% \textcolor{darkblue}{\large \underline{What's wrong}}

%% \begin{itemize}
%% \item SQL isn't expressive enough for physics analysis
%% \item Drill's query planning optimizations are based on SQL's structure
%% \item Drill's native Java is foreign to most of our ecosystem in HEP
%% \end{itemize}

%% \end{columns}

%% \vspace{0.5 cm}
%% \end{frame}

%% \vspace{0.5 cm}
%% Either extend every part of Drill or build a query system with a similar architecture using the same tools (e.g.\ Zookeeper).

\begin{frame}{Toward a query system for HEP}
\vspace{0.5 cm}
\large This talk is about what we would need to make (or alter) a query system for HEP analysis:

\vspace{0.3 cm}
\begin{itemize}\setlength{\itemsep}{0.3 cm}
\item {\bf fast execution} on {\it nested}, non-flat data
\item {\bf distributed processing:} caching and data locality, uneven and changing dataset popularity, histogramming
\item \sout{{\bf HEP-specific query language}} (in the abstract for this talk, but I'm going to focus more on the above)
\end{itemize}
\end{frame}

\begin{frame}
\begin{center}
\huge \textcolor{darkblue}{Fast execution}
\end{center}
\end{frame}

\begin{frame}{The core issue}
\vspace{0.5 cm}
SQL-like query engines are optimized for what we'd call a ``flat ntuple analysis''--- rectangular table of numbers, sliced, filtered, aggregated, and joined.

\vspace{0.5 cm}
\uncover<2->{Only late-stage HEP analysis could fit this model, not AOD.}

\vspace{0.5 cm}
\uncover<3->{General physics analysis requires arbitrary-length lists of objects: e.g.\ events containing jets containing tracks containing hits.}

\vspace{0.5 cm}
\uncover<4->{But frameworks that let the user write object-oriented code are inefficient for queries.}
\end{frame}

\begin{frame}{Illustration}
\vspace{0.5 cm}

\large \textcolor{darkblue}{\underline{Query: fill a histogram with jet $p_T$ of all jets.}}

\begin{center}
\renewcommand{\arraystretch}{1.5}
\small
\begin{tabular}{r l}
\large 0.018 MHz & \large custom CMSSW EDAnalyzer (single-threaded C++) \\
\uncover<5->{\large 0.03 MHz & \large load all 95 jet branches in ROOT} \\
\uncover<4->{\large 2.8 MHz & \large load only jet $p_T$ branch in ROOT} \\
\uncover<3->{\large 12 MHz & \large allocate C++ objects on heap, fill, delete} \\
\uncover<2->{\large 31 MHz & \large allocate C++ objects on stack, fill histogram} \\
\large 250 MHz & \large minimal ``for'' loop (single-threaded C) \\
\end{tabular}
\end{center}

\uncover<6->{\large \textcolor{darkblue}{Four orders of magnitude in performance lost to provide an object-oriented view of the jets, with all attributes filled.}}
\end{frame}

\begin{frame}[fragile]{Alternative}
\vspace{0.5 cm}
Instead of deserializing data into objects, translate the code into loops over raw arrays.

\vspace{0.25 cm}
\textcolor{darkblue}{Turn ``event'' and ``jet'' objects:}

\scriptsize
\begin{minted}{python}
histogram = numpy.zeros(100, dtype=numpy.int32)

def fcn(roottree, histogram):
    for event in roottree:
        for jet in event.jets:
            bin = int(jet.pt)
            if bin >= 0 and bin < 100:
                histogram[bin] += 1
\end{minted}

\vspace{0.25 cm}
\textcolor{darkblue}{\normalsize into ``event'' and ``jet'' array indexes:}

\scriptsize
\begin{minted}{python}
def fcn(eventoffset, jetoffset, jetptdata, histogram):
    for event in range(eventoffset[1]):
        for jet in range(jetoffset[event], jetoffset[event + 1]):
            bin = int(jetptdata[jet])
            if bin >= 0 and bin < 100:
                histogram[bin] += 1
\end{minted}

\end{frame}

\begin{frame}{}

\end{frame}




%% the dream: motivation

%% existence proof: Drill (and others)

%%    * lightweight parallelization
%%    * columnar data
%%    * distributed, in-memory caching
%%    * JIT compilation, vectorization
%%    * data stay on server, only summaries come down

%% pieces we would need: (1) fast execution (2) distributed processing (3) \sout{query language} (in abstract, but focus has turned more to the above)

%% fast execution

%% not better--- best (microbenchmarks to define goal; usefulness of knowing that {\it no} improvement is possible)

%%    * running through data in CMSSW (just an example) vs an HPC-like minimal for loop in C: rate and data touched
%%    * reflects difference in design: CMSSW was {\it meant} to heavy processing on a lot of data per event (reconstruction);
%%      but this makes it a poor choice for lightweight analysis jobs
%%    * analysts mostly use it to dump data to skims, but the slower the framework is per event, the more that needs to be dumped to make it worthwhile (rich get richer)
%%    * lightweight process without persistent skim is a different ``local minimum in ease-of-use space''

%% microbenchmarks are in C, but Python + Numba is also an option: Python is good for informal scripting, developing algorithms quickly, and Numba compiles it with LLVM; runtime is indistinguishable

%% the problem with this is that most of what we want to do with our data doesn't look like simple for loops: we want to pair up particle objects, compute masses, isolations, match reco & MC, etc.; creating objects at runtime is expensive and brings us back to frameworks

%% alternative: instead of making objects at runtime, transform object-oriented code into array manipulation (EXAMPLE)

%% this is formal and general: start with type system and propagate object types through code like a compiler pass, then hand transformed code to Numba for optimization

%%    * P, L, U, R (website)

%% data are in contiguous columnar arrays, like ROOT TBranches, but remain in arrays at runtime

%%    * format allows for random access (EXAMPLE)
%%    * I'm adding TBranch --> Numpy to ROOT to do this on-the-fly (ROOT 6.12)
%%    * but also caching mechanism for more speed and flexibility: the new granularity is not files but columns

%% it's fast, but does it parallelize? yes, up to 2 GB/s per machine (memory bus becomes the bottleneck; plot shows microbenchmark on KNL to avoid memory bus and beat the bottleneck)

%% distributed processing

%% submitting a query and getting back an {\it aggregation} (histogram) is more complex than batch submission

%%    * something in the system must collect partial results from parallel subtasks and add them up
%%    * unavoidable state in the system

%% look at prior art: Hadoop, Spark, and Drill all do this, all are based on Apache Zookeeper

%%    * Zookeeper: distributed, fault-tolerant key-value store for rapidly changing configuration
%%    * ``configuration'' == who's working on what subtask, who's done, which need to be retried, etc.

%% current design (J.Thanat, summer student):

%%    * Zookeeper maintains pool of unfinished subtasks
%%    * workers attempt to claim ownership, preferably if they have input columns in cache (for data locality), but will take anything if they're not busy (to elastically duplicate cache for popular datasets)
%%    * we don't handle {\it any} mutable state ourselves (except volatile cache), no single point of failure (more lessons from industry)

%% query language

%%    * Femtocode is a mini-language based on Python, but with sufficient limitations to allow SQL-like query planning, automated vectorization/GPU translation, and database-style indexing
%%    * but these are all ``2.0'' features

%% conclusions

%%    * AOD-to-plot in seconds {\it is possible}
%%    * they're doing this in industry (but\ldots\ SQL and Java)
%%    * single-threaded kernel is operating in the MHz range (on ``Python'' code)
%%    * prototyping distributed architecture

\end{document}
